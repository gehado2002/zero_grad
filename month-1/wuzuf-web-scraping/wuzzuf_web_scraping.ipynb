{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt26cT5VFYFU",
        "outputId": "094971c1-782e-4c79-8d42-bb6bcbfad3f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping data for 'data analysis'...\n",
            "Scraping data for 'data analysis'...\n",
            "Scraping data for 'data analysis'...\n",
            "Scraping data for 'data analysis'...\n",
            "Scraping data for 'data analysis'...\n",
            "üõ§Ô∏èdata analysis jobs scraped successfully\n",
            "\n",
            "Scraping data for 'data science'...\n",
            "Scraping data for 'data science'...\n",
            "Scraping data for 'data science'...\n",
            "Scraping data for 'data science'...\n",
            "Scraping data for 'data science'...\n",
            "üõ§Ô∏èdata science jobs scraped successfully\n",
            "\n",
            "Scraping data for 'business intelligence'...\n",
            "Scraping data for 'business intelligence'...\n",
            "Scraping data for 'business intelligence'...\n",
            "Scraping data for 'business intelligence'...\n",
            "Scraping data for 'business intelligence'...\n",
            "üõ§Ô∏èbusiness intelligence jobs scraped successfully\n",
            "\n",
            "üòÄDone\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrap():\n",
        "    def web_scraping(job_name,num_pages):\n",
        "\n",
        "        all_titles=[]\n",
        "        all_links=[]\n",
        "        all_companies=[]\n",
        "        all_specs=[]\n",
        "        all_occupations=[]\n",
        "        for num_page in range(num_pages):\n",
        "            url=f'https://wuzzuf.net/search/jobs/?a=hpb&q={job_name.replace(\" \",\"%20\")}\\\n",
        "            %20jobs%20in%20egypt&start={num_page}'\n",
        "            response=requests.get(url)\n",
        "\n",
        "            soup=BeautifulSoup(response.content,'lxml')\n",
        "\n",
        "\n",
        "            titles=soup.find_all(\"h2\",{\"class\":\"css-m604qf\"})\n",
        "            titles=[title.text for title in titles]\n",
        "            all_titles.extend(titles)\n",
        "\n",
        "\n",
        "            links=soup.find_all(\"div\",{\"class\":\"css-pkv5jc\"})\n",
        "            links = [link.a[\"href\"] for link in links if link.a and \"href\" in link.a.attrs]\n",
        "            all_links.extend(links)\n",
        "\n",
        "\n",
        "            occupations=soup.find_all(\"div\",{\"class\":\"css-1lh32fc\"})\n",
        "            occupations=[occupation.text for occupation in occupations ]\n",
        "            all_occupations.extend(occupations)\n",
        "\n",
        "            companies=soup.find_all(\"a\",{\"class\":\"css-17s97q8\"})\n",
        "            companies=[company.text.replace(' -','') for company in companies]\n",
        "            all_companies.extend(companies)\n",
        "\n",
        "\n",
        "            specs=soup.find_all(\"div\",{\"class\":\"css-y4udm8\"})\n",
        "            specs=[spec.text for spec in specs]\n",
        "            all_specs.extend(specs)\n",
        "            print(f\"Scraping data for '{job_name}'...\")\n",
        "        return all_titles, all_links, all_companies, all_specs, all_occupations\n",
        "\n",
        "    for job_name in [\"data analysis\" , \"data science\" , \"business intelligence\"]:\n",
        "            titles, links, companies, specs, occupations= web_scraping(job_name, 5)\n",
        "\n",
        "            dic={}\n",
        "            dic['titles']=titles\n",
        "            dic['links']=links\n",
        "            dic['companies']=companies\n",
        "            dic['specs']=specs\n",
        "            dic['occupations']=occupations\n",
        "\n",
        "            df = pd.DataFrame.from_dict(dic, orient='index').transpose()\n",
        "            df.to_csv(f'{job_name}_jobs_in_egypt.csv',index=False)\n",
        "            print(f\"üõ§Ô∏è{job_name} jobs scraped successfully\\n\")\n",
        "\n",
        "    print(\"üòÄDone\")\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrap()"
      ]
    }
  ]
}